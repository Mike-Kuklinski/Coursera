b <- dt$b
c <- dt$c
str(dt)
l <- list(a,b,c)
dtest <- structure(l, row.names = c(NA,-length(a), class = 'data.table')
)
dtest
dt
str(dtest)
dtest <- structure(l, row.names = c(NA,-length(a), class = 'data.table'))
str(dtest)
dtest <- structure(l, row.names = c(NA,-length(a), class = 'data.frame'))
str(dtest)
l <- list('a' = a, 'b' = b, 'c' = c)
dtest <- structure(l, row.names = c(NA,-length(a), class = 'data.table'))
str(dtest)
dtest <- structure(l, row.names = c(NA,-length(a), class = 'data.frame'))
str(dtest)
dtest <- structure(l, row.names = c(NA,-length(a)), class = 'data.table')
dtest
dtest <- structure(l, row.names = c(NA,-length(a)), class = 'data.frame')
str(dtest)
dtest <- structure(l, row.names = c(NA,-length(a)), class = 'data.table')
str(dtest)
dt
dt[a,]
dt[,a]
remove(a)
dt[a,]
dt[a]
dt[,a]
dt[a==1,]
temp_df <- readRDS('Coursera/Capstone Project/ngrams/model1/ngram_4.rds')
dft <- as.data.table(df)
dft
temp_dt <- as.data.table(temp_df)
dt[a==1,]
dt['a'==1,]
temp_dt[lookup=='why are you',]
temp_dt[lookup=='why are you',]
temp_df[lookup=='why are you',]
temp_df['lookup'=='why are you',]
temp_df[temp_ft$lookup =='why are you',]
temp_df[temp_df$lookup =='why are you',]
t1 <- temp_df[temp_df$lookup =='why are you',]
t1 <- temp_df[temp_df$lookup =='where are you',]
t2 <- temp_dt[lookup =='where are you',]
t2
str(t2)
t2[order(t2$lookup, t2$n, decreasing = T),]
ls(pos = 'package:data.table')
temp_dt[,sum(prob), by=lookup]
temp_dt[,sum(prob), by=lookup]
c <- temp_dt[,sum(prob), by=lookup]
head(c)
c <- temp_dt[,prediction, sum(prob), by=lookup]
c <- temp_dt[,c(prediction, sum(prob)), by=lookup]
names(temp_dt)
c <- temp_dt[,c(predictions, n, sum(prob)), by=lookup]
install.packages('koRpus')
library(koRpus)
library(koRpus)
library(koRpus)
treetag('Kaggle/Home Depot/temp.txt')
treetag('Kaggle/Home Depot/temp.txt', treetagger = )
treetag('Kaggle/Home Depot/temp.txt', treetagger = 'kRp.env')
reactive(tokenize('hello', format="obj", lang='en'))
tokenize('hello', format="obj", lang='en')
?tokenize
tokenize('hello')
t <- 'hello you there'
tokenize(t)
tokenize(txt = t)
tokenize(txt = 't')
tokenize('Kaggle/Home Depot/temp.txt')
tokenize('Kaggle/Home Depot/temp.txt', lang = 'en')
tokenize('Kaggle/Home Depot/temp.txt', lang = 'en', tag = T)
kRp.POS.tags('de')
install.packages('openNLP')
library(openNLP)
tagPOS()
taggedText(t)
tagPOS(t)
library(openNLPdata)
tagPOS(t)
library(NLP)
?taggedText
taggedText(t)
taggedText('Kaggle/Home Depot/temp.txt')
require("NLP")
s <- "hello"
s <- as.String(s)
sent_token_annotator <- Maxent_Sent_Token_Annotator()
word_token_annotator <- Maxent_Word_Token_Annotator()
a2 <- annotate(s, list(sent_token_annotator, word_token_annotator))
a2
pos_tag_annotator <- Maxent_POS_Tag_Annotator()
pos_tag_annotator
a3 <- annotate(s, pos_tag_annotator, a2)
a3
s <- "hello how are you"
s <- as.String(s)
pos_tag_annotator <- Maxent_POS_Tag_Annotator()
pos_tag_annotator
a3 <- annotate(s, pos_tag_annotator, a2)
a3
head(annotate(s, Maxent_POS_Tag_Annotator(probs = TRUE), a2))
a3w <- subset(a3, type == "word")
a3w
tags <- sapply(a3w$features, `[[`, "POS")
tags
table(tags)
sprintf("%s/%s", s[a3w], tags)
a3ws2 <- annotations_in_spans(subset(a3, type == "word"),
subset(a3, type == "sentence")[2L])[[1L]]
sprintf("%s/%s", s[a3ws2], sapply(a3ws2$features, `[[`, "POS"))
a3ws2
a3ws
a3w
require("NLP")
s <- "hello how are you"
s <- as.String(s)
sent_token_annotator <- Maxent_Sent_Token_Annotator()
word_token_annotator <- Maxent_Word_Token_Annotator()
a2 <- annotate(s, list(sent_token_annotator, word_token_annotator))
a2
pos_tag_annotator <- Maxent_POS_Tag_Annotator()
pos_tag_annotator
a3 <- annotate(s, pos_tag_annotator, a2)
a3
head(annotate(s, Maxent_POS_Tag_Annotator(probs = TRUE), a2))
a3w <- subset(a3, type == "word")
tags <- sapply(a3w$features, `[[`, "POS")
tags
table(tags)
sprintf("%s/%s", s[a3w], tags)
t <- c('y' = 'verb', 'ty' = 'noun')
t
library(tm)
stopwords()
pos_dict <- data.frame(
'CD' = 'number',
'JJ' = 'adjective',
'JJR' = 'adjective',
'JJS' = 'adjective',
'NN' = 'noun',
'NNS' = 'noun',
'NNP' = 'noun',
'NNPS' = 'noun',
'RB' = 'adverb',
'RBR' = 'adverb',
'RBS' = 'adverb',
'VB' = 'verb',
'VBD' = 'verb',
'VBG' = 'verb',
'VBN' = 'verb',
'VBP' = 'verb',
'VBZ' = 'verb')
s <- "hello how are you"
require("NLP")
library(NLP)
s <- "table"
pos_tag_annotator <- Maxent_POS_Tag_Annotator()
pos_tag_annotator
a3 <- annotate(s, pos_tag_annotator, a2)
a3
require("NLP")
s <- "table"
pos_tag_annotator <- Maxent_POS_Tag_Annotator()
pos_tag_annotator
a3 <- annotate(s, pos_tag_annotator, a2)
a3
sent_token_annotator <- Maxent_Sent_Token_Annotator()
word_token_annotator <- Maxent_Word_Token_Annotator()
a2 <- annotate(s, list(sent_token_annotator, word_token_annotator))
a2
a3 <- annotate(s, pos_tag_annotator)
s <- "table"
sent_token_annotator <- Maxent_Sent_Token_Annotator()
word_token_annotator <- Maxent_Word_Token_Annotator()
a2 <- annotate(s, list(sent_token_annotator, word_token_annotator))
pos_tag_annotator <- Maxent_POS_Tag_Annotator()
pos_tag_annotator
a3 <- annotate(s, pos_tag_annotator, a2)
a3
temp_word <- "table"
sent_token_annotator <- Maxent_Sent_Token_Annotator()
word_token_annotator <- Maxent_Word_Token_Annotator()
word_t <- annotate(word, list(sent_token_annotator, word_token_annotator))
word <- "table"
sent_token_annotator <- Maxent_Sent_Token_Annotator()
word_token_annotator <- Maxent_Word_Token_Annotator()
word_t <- annotate(word, list(sent_token_annotator, word_token_annotator))
word <- annotate(word, Maxent_POS_Tag_Annotator(), word_t)
word
word <- "table"
word_t <- annotate(word, list(Maxent_Sent_Token_Annotator(),
Maxent_Word_Token_Annotator()))
word <- annotate(word, Maxent_POS_Tag_Annotator(), word_t)
word
head(annotate(word, Maxent_POS_Tag_Annotator(probs = TRUE), word_t))
word <- subset(word, type == "word")
word
word
word$features
word$features[1]
word$features[[1]]
word$features[[1]][1]
word$features[[1]]
word$POS
y <- word$features[[1]]
y
y[1]
y[[1]]
y <- word$features[[1]][[1]]
y
word <- "tape"
word_t <- annotate(word, list(Maxent_Sent_Token_Annotator(),
Maxent_Word_Token_Annotator()))
word <- annotate(word, Maxent_POS_Tag_Annotator(), word_t)
word <- subset(word, type == "word")
word_pos <- word$features[[1]][[1]]
word_pos
word <- "swam"
word_t <- annotate(word, list(Maxent_Sent_Token_Annotator(),
Maxent_Word_Token_Annotator()))
word <- annotate(word, Maxent_POS_Tag_Annotator(), word_t)
word <- subset(word, type == "word")
word_pos <- word$features[[1]][[1]]
word_pos
word <- "swam over"
word_t <- annotate(word, list(Maxent_Sent_Token_Annotator(),
Maxent_Word_Token_Annotator()))
word <- annotate(word, Maxent_POS_Tag_Annotator(), word_t)
word <- subset(word, type == "word")
word_pos <- word$features[[1]][[1]]
word_pos
word
word <- "scotch tape"
word_t <- annotate(word, list(Maxent_Sent_Token_Annotator(),
Maxent_Word_Token_Annotator()))
word <- annotate(word, Maxent_POS_Tag_Annotator(), word_t)
word <- subset(word, type == "word")
word_pos <- word$features[[1]][[1]]
word_pos
word
word <- "tape the show"
word_t <- annotate(word, list(Maxent_Sent_Token_Annotator(),
Maxent_Word_Token_Annotator()))
word <- annotate(word, Maxent_POS_Tag_Annotator(), word_t)
word <- subset(word, type == "word")
word_pos <- word$features[[1]][[1]]
word_pos
word
word$features[[1]][[1]]
word$features[[1]][[2]]
word$features[[2]][[1]]
check_text <- 'table chairs best world complaining'
query_str <- 'table chairs best world complaining'
word_idx <- 1
noun_cnt <- 0
adjective_cnt <- 0
verb_cnt <- 0
adverb_cnt <- 0
number_cnt <- 0
query_str <- unlist(str_split(remove_stopwords(query_str), pattern = ' '))
library(caret)
library(tm)
library(dplyr)
library(stringr)
library(data.table)
library(koRpus)
library(NLP)
query_str <- unlist(str_split(remove_stopwords(query_str), pattern = ' '))
get_brand <- function(input_str){
unlist(str_split(str_trim(input_str), pattern = ' '))[1]
}
# Determine if brand name is mentioned in query
check_brand <- function(query, brand_name){
grepl(brand_name, query, ignore.case = T)
}
# Remove common stopwords from descriptions
remove_stopwords <- function(input_str){
input_str <- unlist(str_split(str_trim(tolower(input_str)), pattern = ' '))
stop_list <- which(input_str %in% stopwords(kind = 'en'))
input_str <- paste(input_str[-stop_list], collapse = ' ')
input_str <- gsub('/', ' ', input_str)
input_str <- gsub('\\?', '', input_str)
input_str
}
query_str <- unlist(str_split(remove_stopwords(query_str), pattern = ' '))
query_str
query_str <- 'table chairs best world complaining'
check_text <- 'table chairs best world complaining'
word_idx <- 1
query_str <- unlist(str_split(remove_stopwords(query_str), pattern = ' '))
query_str <- 'table chairs best world complaining'
remove_stopwords(query_str)
query_str <- 'table chairs best world complaining'
input_str <- query_str
remove_stopwords(input_str)
query_str <- 'table chairs best world complaining'
input_str <- query_str
input_str <- unlist(str_split(str_trim(tolower(input_str)), pattern = ' '))
stop_list <- which(input_str %in% stopwords(kind = 'en'))
input_str <- paste(input_str[-stop_list], collapse = ' ')
stop_list
stop_list == 0
is.null(stop_list)
stop_list[1]
is.na(stop_list[1])
query_str <- 'table chairs best world complaining'
input_str <- query_str
input_str <- unlist(str_split(str_trim(tolower(input_str)), pattern = ' '))
stop_list <- which(input_str %in% stopwords(kind = 'en'))
if(is.na(stop_list[1]) == F){
input_str <- paste(input_str[-stop_list], collapse = ' ')
}else{
input_str <- paste(input_str, collapse = ' ')
}
input_str <- gsub('/', ' ', input_str)
input_str <- gsub('\\?', '', input_str)
input_str
remove_stopwords <- function(input_str){
input_str <- unlist(str_split(str_trim(tolower(input_str)), pattern = ' '))
stop_list <- which(input_str %in% stopwords(kind = 'en'))
if(is.na(stop_list[1]) == F){
input_str <- paste(input_str[-stop_list], collapse = ' ')
}else{
input_str <- paste(input_str, collapse = ' ')
}
input_str <- gsub('/', ' ', input_str)
input_str <- gsub('\\?', '', input_str)
input_str
}
query_str <- 'table chairs best world complaining'
check_text <- 'table chairs best world complaining'
word_idx <- 1
query_str <- unlist(str_split(remove_stopwords(query_str), pattern = ' '))
pos_types <- get_pos(query_str)
get_pos <- function(query){
query_t <- annotate(query, list(Maxent_Sent_Token_Annotator(),
Maxent_Word_Token_Annotator()))
query <- annotate(query, Maxent_POS_Tag_Annotator(), query_t)
query <- subset(query, type == "word")
query
}
pos_types <- get_pos(query_str)
query_str
pos_types <- get_pos(paste(query_str, collapse = ' '))
paste(query_str, collapse = ' ')
query_str <- 'table chairs best world complaining'
get_pos(query_str)
query_str <- 'table chairs best world complaining'
query <- query_str
query_t <- annotate(query, list(Maxent_Sent_Token_Annotator(),
Maxent_Word_Token_Annotator()))
query <- as.string(query_str)
query <- as.String(query_str)
query_t <- annotate(query, list(Maxent_Sent_Token_Annotator(),
Maxent_Word_Token_Annotator()))
query
query_t <- annotate(query, list(Maxent_Sent_Token_Annotator(),
Maxent_Word_Token_Annotator()))
word <- 'hello'
word <- "tape the show"
word_t <- annotate(word, list(Maxent_Sent_Token_Annotator(),
Maxent_Word_Token_Annotator()))
library(NLP)
word <- "tape the show"
word_t <- annotate(word, list(Maxent_Sent_Token_Annotator(),
Maxent_Word_Token_Annotator()))
require("NLP")
word <- "tape the show"
word_t <- annotate(word, list(Maxent_Sent_Token_Annotator(),
Maxent_Word_Token_Annotator()))
library(openNLP)
query_t <- annotate(query, list(Maxent_Sent_Token_Annotator(),
Maxent_Word_Token_Annotator()))
s <- 'hello you there'
sent_token_annotator <- Maxent_Sent_Token_Annotator()
word_token_annotator <- Maxent_Word_Token_Annotator()
a2 <- annotate(s, list(sent_token_annotator, word_token_annotator))
s <- 'hello you there'
s <- as.String(s)
sent_token_annotator <- Maxent_Sent_Token_Annotator()
word_token_annotator <- Maxent_Word_Token_Annotator()
a2 <- annotate(s, list(sent_token_annotator, word_token_annotator))
require("NLP")
s <- 'hello you there'
s <- as.String(s)
sent_token_annotator <- Maxent_Sent_Token_Annotator()
word_token_annotator <- Maxent_Word_Token_Annotator()
a2 <- annotate(s, list(sent_token_annotator, word_token_annotator))
setwd('~/R Scripts/Coursera/Practical Machine Learning/Course Project')
library(caret)
library(ggplot2)
library(randomForest)
library(gbm)
best_model <- readRDS("./Models/bestmodel.rds")
Tdata <- read.csv('./rawdata/TrainExer.csv')
zero_check <- nearZeroVar(Tdata, saveMetrics = TRUE)
kurt <- grep('^kurtosis',names(Tdata), value = F)
skew <- grep('^skewness',names(Tdata), value = F)
maxi <- grep('^max',names(Tdata), value = F)
mini <- grep('^min',names(Tdata), value = F)
amp <- grep('^amplitude',names(Tdata), value = F)
vari <- grep('^var',names(Tdata), value = F)
avg <- grep('^avg',names(Tdata), value = F)
std <- grep('^stddev',names(Tdata), value = F)
time <- c(1, 2, 3, 4, 5, 6, 7)
remove_index<-sort(c(time, kurt, skew, maxi, mini, amp, vari, avg, std))
subTdata <- Tdata[,-remove_index]
remove(time, kurt, skew, maxi, mini, amp, vari, avg, std)
integers <- sapply(subTdata, is.integer)
tonum <- which(integers == T)
for(i in seq_along(tonum)){
subTdata[,tonum[i]] <- as.numeric(subTdata[,tonum[i]])
}
cor_subTdata <- abs(cor(subTdata[,-53]))
diag(cor_subTdata) <- 0
highcor <- which(cor_subTdata > .8, arr.ind = T)
num_high_cor <- nrow(highcor)
pp <- preProcess(subTdata[,-53], method = 'pca', thresh = 0.99)
pp_subTdata <- predict(pp, subTdata[,-53])
pp_subTdata <- cbind(pp_subTdata, classe = subTdata$classe)
Tstdata <- read.csv('./rawdata/TestExer.csv')
subTstdata <- Tstdata[,-remove_index]
integers <- sapply(subTstdata, is.integer)
tonum <- which(integers == T)
for(i in seq_along(tonum)){
subTstdata[,tonum[i]] <- as.numeric(subTstdata[,tonum[i]])
}
pp_subTstdata <- predict(pp, subTstdata[,-53])
pp_subTstdata <- cbind(pp_subTstdata,
classe = subTstdata$problem_id)
test_pred <- predict(best_model, pp_subTstdata)
test_pred
best_model
View(highcor)
set.seed(45)
subsub <- sample(1:nrow(pp_subTdata), 1000)
T1 <- pp_subTdata[subsub,]
Model_rpart <- train(classe ~ . , data = T1, method = 'rpart')
Model_rf <- train(classe ~ . , data = T1, method = 'rf')
Model_gbm <- train(classe ~ . , data = T1, method = 'gbm')
saveRDS(Model_rpart, "./Models/Model_rpart.rds")
saveRDS(Model_rf, "./Models/Model_rf.rds")
saveRDS(Model_gbm, "./Models/Model_gbm.rds")
Model_rpart
Model_rpart$finalModel
Model_gbm
Model_rf
best_model
best_model$finalModel
setwd('Weight_Lifting_Class_Prediction_Model/')
suppressMessages(suppressWarnings(library(caret)))
suppressMessages(suppressWarnings(library(ggplot2)))
# create folder to store and work with data
setwd('~/R Scripts/Coursera/Practical Machine Learning/Course Project/Weight_Lifting_Class_Prediction_Model/')
if (!dir.exists('rawdata')){
dir.create('rawdata')
url <- 'https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv'
download.file(url,destfile='./rawdata/TrainExer.csv')
}
Tdata <- read.csv('./rawdata/TrainExer.csv')
str(Tdata)
zero_check <- nearZeroVar(Tdata, saveMetrics = TRUE)
head(subset(zero_check, nzv == T), 15)
kurt <- grep('^kurtosis',names(Tdata), value = F)
skew <- grep('^skewness',names(Tdata), value = F)
maxi <- grep('^max',names(Tdata), value = F)
mini <- grep('^min',names(Tdata), value = F)
amp <- grep('^amplitude',names(Tdata), value = F)
vari <- grep('^var',names(Tdata), value = F)
avg <- grep('^avg',names(Tdata), value = F)
std <- grep('^stddev',names(Tdata), value = F)
time <- c(1, 2, 3, 4, 5, 6, 7)
remove_index<-sort(c(time, kurt, skew, maxi, mini, amp, vari, avg, std))
# Remove columns from data and delete temp variables
subTdata <- Tdata[,-remove_index]
remove(time, kurt, skew, maxi, mini, amp, vari, avg, std)
dim(subTdata)
integers <- sapply(subTdata, is.integer)
tonum <- which(integers == T)
for(i in seq_along(tonum)){
subTdata[,tonum[i]] <- as.numeric(subTdata[,tonum[i]])
}
cor_subTdata <- abs(cor(subTdata[,-53]))
diag(cor_subTdata) <- 0
highcor <- which(cor_subTdata > .8, arr.ind = T)
nrow(highcor)
pp <- preProcess(subTdata[,-53], method = 'pca', thresh = 0.99)
pp_subTdata <- predict(pp, subTdata[,-53])
pp_subTdata <- cbind(pp_subTdata, classe = subTdata$classe)
pp
g2 <- ggplot(data = pp_subTdata, aes(x = PC1, y = PC2, col = classe))
g2 <- g2 + geom_point()
g2
set.seed(45)
subsub <- sample(1:nrow(pp_subTdata), 1000)
T1 <- pp_subTdata[subsub,]
Model_rf <- readRDS("./Models/Model_rf.rds")
Model_rf
best_model <- readRDS("./Models/bestmodel.rds")
best_model$finalModel
getwd()
best_model <- readRDS("Models/bestmodel.rds")
best_model$finalModel
best_model$finalModel
best_model$method
best_model$finalModel
